{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Feature Linear Regression\n",
    "\n",
    "The problems in which there are more than one independent feature (variables) that can be used to predict the output.\n",
    "\n",
    "Referring the house price prediction problem:\n",
    "\n",
    "|Size in sqft   |No of bedroom  |No. of floors  |Age of home in years   | Price ($) in $1000's  |\n",
    "|:-------------:|:-------------:|:-------------:|:---------------------:|:---------------------:|\n",
    "|$X_1$          |$X_2$          |$X_3$          |$X_4$                  |                       |\n",
    "|2104           |5              |1              |45                     |460                    |\n",
    "|1416           |3              |2              |40                     |232                    |\n",
    "|852            |2              |1              |36                     |178                    |\n",
    "\n",
    "- $x_j$ = $j^{th}$ feature\n",
    "- n = number of features\n",
    "- $x^{(i)}$ - features of $i^{th}$ training example i.e. $(x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x_{j}^{(i)}$ - value of features j in $i^{th}$ training example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated model prediction function with multiple variables\n",
    "\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the input training data\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_feature_model_function(x, w, b):\n",
    "    \"\"\"\n",
    "    Model prediction function using the dot product of X and w\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Slower approach\n",
    "    Iterates m times to calculate product of each feature x and w one by one\n",
    "    adding it to the prediction\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        n = x[i].shape[0]\n",
    "        p_i = 0\n",
    "        for j in range(n):\n",
    "            p_ij = x[i,j] * w[j]\n",
    "            p_i = p_i + p_ij\n",
    "\n",
    "        p[i] = p_i + b\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Faster approach using prebuilt np.dot() function.\n",
    "    It uses hardware parallel processing to compute the products of each item in the vector\n",
    "    and then sum it using efficient algorithm\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize w and b\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_wb shape (3,), prediction: [459.99999762 231.99999837 177.99999899]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction from the training data\n",
    "f_wb = multi_feature_model_function(X_train, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost With Multiple Variables\n",
    "\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to single feature, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are now vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0\n",
    "    m = X.shape[0]\n",
    "    y_predictions = multi_feature_model_function(X, w, b)\n",
    "\n",
    "    for i in range(m):\n",
    "        diff = y_predictions[i] - y[i]\n",
    "        cost = cost + diff ** 2\n",
    "    cost = cost / (2 * m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of model function when \n",
      "w = [  0.39133535  18.75376741 -53.36032453 -26.42131618] and \n",
      "b = 785.1811367994083 is \n",
      "1.5578904045996674e-12\n"
     ]
    }
   ],
   "source": [
    "# Computing the cost for the initialized w and b\n",
    "\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f\"Cost of model function when \\nw = {w_init} and \\nb = {b_init} is \\n{cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent With Multiple Variables\n",
    "Equation of the Gradient descent for multiple variables: \n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "\n",
    "    y_predictions = multi_feature_model_function(X, w, b)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + ((y_predictions[i] - y[i]) * X[i][j])\n",
    "        dj_db = dj_db + (y_predictions[i] - y[i])\n",
    "\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w = [  0.39133535  18.75376741 -53.36032453 -26.42131618] \n",
      "b = 785.1811367994083 is \n",
      "dj_dw = [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05] \n",
      "dj_db = -1.6739251122999121e-06\n"
     ]
    }
   ],
   "source": [
    "dw_init, db_init = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f\"Gradient of w = {w_init} \\nb = {b_init} is \\ndj_dw = {dw_init} \\ndj_db = {db_init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha=0.001, num_iters=100000):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w, b for learning rate 0.0000005000 and 1000 iterations is \n",
      "w = [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ]\n",
      "b = -0.00\n",
      "\n",
      "Predictions after setting final w and b\n",
      "[426.18530497 286.16747201 171.46763087]\n"
     ]
    }
   ],
   "source": [
    "temp_w = np.zeros(w_init.shape[0])\n",
    "temp_b = 0\n",
    "\n",
    "# Learning rate\n",
    "alpha = 5.0e-7\n",
    "\n",
    "# Number of iterations\n",
    "num_iters = 10 ** 3\n",
    "\n",
    "# Finding final w, b using the gradient descent algorithm\n",
    "w_final, b_final = gradient_descent(X_train, y_train, temp_w, temp_b, alpha=alpha, num_iters=num_iters)\n",
    "\n",
    "print(f\"Final w, b for learning rate {alpha:0.10f} and {num_iters} iterations is \\nw = {w_final}\\nb = {b_final:0.2f}\\n\")\n",
    "\n",
    "print(f\"Predictions after setting final w and b\\n{multi_feature_model_function(X_train, w_final, b_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
